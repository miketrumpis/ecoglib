{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods: Bootstrap and Jackknife"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "ecoglib implements samplers for two types of resampling methods: :py:class:`ecoglib.estimation.resampling.Bootstrap` and :py:class:`ecoglib.estimation.resampling.Jackknife`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(108774)\n",
    "import scipy.stats.distributions as dists\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import ecoglib.estimation.resampling as resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple bootstrapping\n",
    "\n",
    "Both resampling schemes are *nonparameteric* methods to get statistics of a sampling distribution. This is in contrast to a parametric estimator analysis that pre-supposes the sampling distribution. \n",
    "\n",
    "**Parametric analysis**\n",
    "\n",
    "A simple parametric assumption would be a sample $\\{x\\}_{i}$ of $N$ independent and identically distributed (iid) variates from a friendly distribution with (*finite!*) mean $\\mu$ and variance $\\sigma^{2}$. It would follow that the sample mean estimator $\\hat{\\mu}=\\sum_{i}(N^{-1}x_{i})$ is a variate with mean $\\mu$ and variance $\\sigma^{2}/N$. In other words, the standard error (SE) of the mean (SEM) is $\\sigma /\\sqrt{N}$ (which is itself has an unbiased estimate using the sample variance estimator $\\hat{\\sigma}^{2}/(N-1)$).\n",
    "\n",
    "**Nonparametric (bootstrap) analysis**\n",
    "\n",
    "[Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29) tries to infer the sample distribution by creates many point estimates (usually >>100) from synthetic samples. Specifically, bootstrap drawns $N$ times from the original $N$ points with replacement to emulate repeating an experiment to draw $N$ *new* points. The quantiles of the $P$ pseudo estimates are then used to infer statistics of the actual sample distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 13   # sample size\n",
    "P = 1000  # resamples\n",
    "# normal samples mean of 5, stdev 2\n",
    "sigma_sq = 4\n",
    "mu = 5\n",
    "sample_dist = dists.norm(loc=mu, scale=sigma_sq ** 0.5)\n",
    "x_norm = sample_dist.rvs(size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = resampling.Bootstrap(x_norm, P, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('Real sample:', np.sort(x_norm))\n",
    "    for n, samp in enumerate(bs.sample()):\n",
    "        print('Fake sample {}:'.format(n + 1), np.sort(samp))\n",
    "        if n == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw all bootstrap samples and look at the apparent distribution of the mean estimator compared to the actual distribution of the mean estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bs = bs.all_samples(estimator=np.mean)  # pseudo-samples from distribution\n",
    "sample_mean = np.mean(mean_bs)\n",
    "mean_dist = dists.norm(mu, (sigma_sq / N) ** 0.5)  # actual distribution\n",
    "x = np.linspace(3, 7, 100)\n",
    "px = mean_dist.pdf(x)\n",
    "actual_CI = mean_dist.ppf([0.025, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap samples are only as good as the original sample. With a fairly low $N$, we'd expect a poor mean estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(mean_bs, bins='auto', density=True, histtype='step')\n",
    "_ = plt.plot(x, px)\n",
    "_ = plt.axvline(sample_mean, color='r', label='mean est.')\n",
    "_ = plt.axvline(actual_CI[0], color='gray', ls='--')\n",
    "_ = plt.axvline(actual_CI[1], color='gray', ls='--', label='Actual CI')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the $P$ estimator replicates, we can compare the nonparametric quantiles of the mean to the parametric SEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean = x_norm.mean()\n",
    "# shift the actual CI to be centered around the sample mean\n",
    "shift_CI = actual_CI - 5 + sample_mean\n",
    "# unbiased sem estimator\n",
    "parametric_sem = x_norm.std() / np.sqrt(N - 1)\n",
    "parametric_CI = dists.norm.ppf([0.025, 0.975], loc=sample_mean, scale=parametric_sem)\n",
    "bootstrap_CI = np.percentile(mean_bs, [2.5, 97.5])\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(mean_bs, bins='auto', density=True, histtype='step')\n",
    "_ = plt.axvline(sample_mean, color='r', label='mean est.')\n",
    "_ = plt.axvline(shift_CI[0], color='gray', ls='--')\n",
    "_ = plt.axvline(shift_CI[1], color='gray', ls='--', label='mean est. +/- actual CI')\n",
    "_ = plt.axvline(parametric_CI[0], color='darkorange', ls='--')\n",
    "_ = plt.axvline(parametric_CI[1], color='darkorange', ls='--', label='+/- parametric CI')\n",
    "_ = plt.axvline(bootstrap_CI[0], color='yellowgreen', ls='--')\n",
    "_ = plt.axvline(bootstrap_CI[1], color='yellowgreen', ls='--', label='+/- bootstrap CI')\n",
    "plt.legend(loc=(1.1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife\n",
    "\n",
    "The [jackknife](https://en.wikipedia.org/wiki/Jackknife_resampling) predates bootstrap as a resampling method. It uses a leave-one-out resampling scheme, rather than resampling with replacement, which means that it can create only (up to) $N$ replicate estimators. For the sample $X_{i}$ (set notation omitted for simplicity), each jackknifed sample is\n",
    "\n",
    "$X_{-i}=(x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_{N})$\n",
    "\n",
    "\n",
    "The jackknife is used to find the mean and error of not-necessarily-simple estimators $\\theta(X)$. However, it is often warned that the jackknife should be avoided for nonlinear, non-smooth estimators, such as median and correlation coefficient. The jackknife estimate of the mean is defined as\n",
    "\n",
    "$\\hat{\\theta}_{jn}=\\frac{1}{N}\\sum_i \\theta(X_{-i})$\n",
    "\n",
    "The jackknife does have the advantage of correcting biased estimators (to the leading $1/N$ order of its Taylor expansion). For an estimator $\\theta(X)$ that is biased, we can form jackknife *pseudovalues* from the estimator $\\theta_{-i}$ applied to each jackknife sample:\n",
    "\n",
    "$\\tilde{\\theta}_{-i}=N\\bar{\\theta}-(N-1)\\theta_{-i}$\n",
    "\n",
    "(where $\\bar{\\theta}$ is estimated from the *entire* sample). The average of the pseudovalues, which is $N\\bar{\\theta}-(N-1)\\hat{\\theta}_{jn}$, is a bias-corrected version of the estimator $\\theta$.\n",
    "\n",
    "The jackknife standard error is defined two ways. The basic jackknife estimator variance is based on jackknifed estimators $\\theta_{-i}$\n",
    "\n",
    "$SE_{1}=\\sqrt{\\frac{N-1}{N}\\sum_{i}(\\theta_{-i}-\\hat{\\theta}_{jn})^2}$\n",
    "\n",
    "When using pseudovalues, the jackknife SE is\n",
    "\n",
    "$SE_{2}=\\sqrt{\\left(N(N-1)\\right)^{-1}\\sum_{i}(\\tilde{\\theta}_{-i}-\\langle \\tilde{\\theta} \\rangle)^{2}}$\n",
    "\n",
    "where $\\langle \\rangle$ notation denotes the sample mean of pseudovalues.\n",
    "\n",
    "### Bias example: sample variance\n",
    "The maximum likelihood variance estimator is $N^{-1}\\sum_{i}(x_{i}-\\bar{x})^{2}$, which has a famous bias of $-\\sigma^{2}/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svar(x):\n",
    "    return np.mean((x - np.mean(x)) ** 2)\n",
    "\n",
    "N = 15   # sample size\n",
    "# normal samples mean of 5, stdev 2\n",
    "sigma_sq = 4\n",
    "mu = 5\n",
    "sample_dist = dists.norm(loc=mu, scale=sigma_sq ** 0.5)\n",
    "x_norm = sample_dist.rvs(size=N)\n",
    "\n",
    "S2 = svar(x_norm)\n",
    "\n",
    "jn = resampling.Jackknife(x_norm, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Jackknife`` object can be used to estimate the bias of ``svar``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jn_bias = jn.bias(svar)\n",
    "known_bias = -sigma_sq / N\n",
    "print('Jackknife bias: {} (actual bias {})'.format(jn_bias, known_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jackknife estimate can be made with or without bias correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ML estimator:', S2)\n",
    "print('JN estimator:', jn.estimate(svar, correct_bias=False))\n",
    "print('JN bias corrected:', jn.estimate(svar, correct_bias=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the variance estimator itself has error, which is fairly high compared to the bias. The error can be computed directly (jackknife variance), or returned at the same time as the mean estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jackknife SE:', jn.variance(svar) ** 0.5)\n",
    "print('Jackknife est and SE:', *jn.estimate(svar, se=True, correct_bias=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More common use: optimization confidence intervals\n",
    "\n",
    "Bootstrap is usually used for complex sampling distributions. For example, we can get a confidence interval for parameters that are derived from optimization problems. We can demonstrate this using ordinary least squares (OLS), which again has an easy to compare parametric analysis. (Again, the bootstrap would normally be used for a more complicated distribution, e.g. for optimizing a nonlinear least squares problem.)\n",
    "\n",
    "Here we have imagined observations of a dependent variable $y$ that linearly varies with the independent variable $x$ with a factor of 1.7. The observations have independent zero mean error with 30% variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(55)\n",
    "e = np.random.randn(55) * (0.3 ** 0.5)\n",
    "y = 1.7 * x + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('IV')\n",
    "plt.ylabel('DV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit an OLS model (slope & intercept) using [statsmodels](https://www.statsmodels.org/dev/user-guide.html#regression-and-linear-models). The size of the confidence intervals here are based on parametric analysis and, interestingly, *only* depend on the independent variable (and not the observations at all!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_fm = sm.OLS(y, np.c_[np.ones_like(x), x]).fit()\n",
    "ols_fm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the summary states, the 95% CI for the intercept marks it as not necessarily non-zero. The 95% CI slope is near the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_fm.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_fm.scale, ols_fm.ssr / ols_fm.df_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on parametric analysis, the predicted fit has a confidence interval that depends on the estimated error (``ols_fm.scale``) and the \"residual degrees of freedom\" (N - size-of-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the model for 100 points\n",
    "xm = np.linspace(x.min(), x.max(), 100)\n",
    "pred = ols_fm.get_prediction(np.c_[np.ones_like(xm), xm])\n",
    "pframe = pred.summary_frame(alpha=0.05)\n",
    "ym = pframe.mean\n",
    "ym_lo = pframe.mean_ci_lower\n",
    "ym_hi = pframe.mean_ci_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = np.linspace(x.min(), x.max(), 100)\n",
    "ym = ols_fm.predict(np.c_[np.ones_like(xm), xm])\n",
    "plt.figure()\n",
    "plt.scatter(x, y, color='k', label='data')\n",
    "plt.plot(xm, ym, color='r', label='predicted mean')\n",
    "plt.plot(xm, np.c_[ym_lo, ym_hi], color='slategray', ls='--', label='95% CI for mean')\n",
    "plt.xlabel('IV')\n",
    "plt.ylabel('DV')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get nonparametric confidence intervals, we can bootstrap the OLS fit and check the quantiles of the resulting optimizations. First, define an estimator function that returns optimization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ols(x, y, axis=None):\n",
    "    fm = sm.OLS(y, np.c_[np.ones_like(x), x]).fit()\n",
    "    return fm.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a bootstrapper that resamples BOTH $x$ and $y$ with *the same permutations*. (Another note here: the ``Bootstrap`` object pre-computes the sampling permutations so that work on each sample can be mapped out to subprocesses. Use ``n_jobs=None``, or ``n_jobs=X`` to take advantage of multiple parallel jobs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapper = resampling.Bootstrap([x, y], 1000)\n",
    "fits = np.array([p for p in bootstrapper.sample(estimator=compute_ols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average fit and the bootstrapped CI are pretty close to the parametric CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mn = fits.mean(axis=0)\n",
    "p_lo, p_hi = np.percentile(fits, [2.5, 97.5], axis=0)\n",
    "print('Slope BS: {:.3f} ({:.3f} - {:.3f} 95% CI)'.format(p_mn[1], p_lo[1], p_hi[1]))\n",
    "print('Intercept BS: {:.3f} ({:.3f} - {:.3f} 95% CI)'.format(p_mn[0], p_lo[0], p_hi[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take these bootstrapped fits to make a CI for the predicted trend. Create a curve for each bootstrap fit and then find the quantiles of these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xm * fits[:, 1, None] + fits[:, 0, None]\n",
    "ym_lo_bs, ym_hi_bs = np.percentile(predictions, [2.5, 97.5], axis=0)\n",
    "ym_bs = predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = np.linspace(x.min(), x.max(), 100)\n",
    "plt.figure()\n",
    "plt.scatter(x, y, color='k', label='data')\n",
    "plt.plot(xm, ym_bs, color='r', label='BS mean')\n",
    "plt.plot(xm, np.c_[ym_lo_bs, ym_hi_bs], color='slategray', ls='--', label='BS 95% CI')\n",
    "plt.xlabel('IV')\n",
    "plt.ylabel('DV')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing estimates in parallel\n",
    "\n",
    "Resampling methods can be resource intensive for any but the most trivial estimators, but the repeated function calls can be done independently on multiple processes. **Due to the overhead involved in creating new processes, going parallel would only make sense for costly estimation functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "\n",
    "\n",
    "def slow_mean(x):\n",
    "    mu = x.mean()\n",
    "    sleep(0.05)\n",
    "    return mu\n",
    "\n",
    "\n",
    "# reuse the normal distribution\n",
    "N = int(5e6)\n",
    "x_norm = sample_dist.rvs(size=N)\n",
    "\n",
    "# run in a single process\n",
    "t1 = time()\n",
    "mu_est = resampling.Bootstrap.bootstrap_estimate(x_norm, 100, slow_mean, n_jobs=1)\n",
    "tc = time() - t1\n",
    "print('mu estimate: {} ({} sec)'.format(mu_est, tc))\n",
    "\n",
    "# run in 6 processes\n",
    "t1 = time()\n",
    "mu_est = resampling.Bootstrap.bootstrap_estimate(x_norm, 100, slow_mean, n_jobs=6)\n",
    "tc = time() - t1\n",
    "print('mu estimate: {} ({} sec)'.format(mu_est, tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the normal distribution\n",
    "N = int(1e6)\n",
    "x_norm = sample_dist.rvs(size=N)\n",
    "\n",
    "t1 = time()\n",
    "bs = resampling.Bootstrap(x_norm, 1000, n_jobs=1)\n",
    "all_samps1 = bs.all_samples()\n",
    "tc = time() - t1\n",
    "print('serial collect: {} sec'.format(tc))\n",
    "\n",
    "t1 = time()\n",
    "bs = resampling.Bootstrap(x_norm, 1000, n_jobs=6)\n",
    "all_samps2 = bs.all_samples()\n",
    "tc = time() - t1\n",
    "print('para collect: {} sec'.format(tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the normal distribution\n",
    "N = int(1e6)\n",
    "x_norm = sample_dist.rvs(size=N)\n",
    "\n",
    "t1 = time()\n",
    "bs = resampling.Bootstrap(x_norm, 1000, n_jobs=1, alloc_shared=False)\n",
    "all_samps1 = bs.all_samples()\n",
    "tc = time() - t1\n",
    "print('serial collect: {} sec'.format(tc))\n",
    "\n",
    "t1 = time()\n",
    "bs = resampling.Bootstrap(x_norm, 1000, n_jobs=6, alloc_shared=False)\n",
    "all_samps2 = bs.all_samples()\n",
    "tc = time() - t1\n",
    "print('para collect: {} sec'.format(tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_samps1\n",
    "del all_samps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "while gc.collect():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.runctx(\"bs = resampling.Bootstrap(x_norm, 1000, n_jobs=1); all_samps1 = bs.all_samples()\", globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.runctx(\"bs = resampling.Bootstrap(x_norm, 1000, n_jobs=6); all_samps1 = bs.all_samples()\", globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.runctx(\"bs = resampling.Bootstrap(x_norm, 1000, n_jobs=6, alloc_shared=False); all_samps1 = bs.all_samples()\", globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
